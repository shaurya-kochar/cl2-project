{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a651d3",
   "metadata": {},
   "source": [
    "# Notebook 02: Preprocessing Pipeline\n",
    "\n",
    "This notebook demonstrates the preprocessing pipeline implemented in `src/preprocess.py`.\n",
    "\n",
    "**Pipeline Steps**:\n",
    "1. Text cleaning (HTML removal, URL removal, lowercasing, normalization)\n",
    "2. Tokenization (NLTK word_tokenize)\n",
    "3. POS tagging (NLTK pos_tag)\n",
    "4. Feature extraction (length, punctuation, negation, type-token ratio)\n",
    "\n",
    "**Output**: `data/processed.csv` with enriched metadata columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced65633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from preprocess import clean_text, tokenize_text, pos_tag_tokens, punctuation_count, negation_count, type_token_ratio\n",
    "\n",
    "pd.set_option('display.max_colwidth', 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc38d1",
   "metadata": {},
   "source": [
    "## 1. Test Individual Preprocessing Functions\n",
    "\n",
    "Let's test each preprocessing step on sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "<br /><br />This movie was NOT good at all! I can't believe I wasted my time. \n",
    "Visit http://example.com for more terrible reviews. The acting wasn't great either...\n",
    "\"\"\"\n",
    "\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "cleaned = clean_text(sample_text, keep_punct=True)\n",
    "print(\"CLEANED TEXT:\")\n",
    "print(cleaned)\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "tokens = tokenize_text(cleaned)\n",
    "print(f\"TOKENS ({len(tokens)} total):\")\n",
    "print(tokens[:20])\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "pos_tags = pos_tag_tokens(tokens)\n",
    "print(f\"POS TAGS ({len(pos_tags)} total):\")\n",
    "print(list(zip(tokens[:15], pos_tags[:15])))\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"EXTRACTED FEATURES:\")\n",
    "print(f\"  Punctuation count: {punctuation_count(sample_text)}\")\n",
    "print(f\"  Negation count: {negation_count(tokens)}\")\n",
    "print(f\"  Type-Token Ratio: {type_token_ratio(tokens):.3f}\")\n",
    "print(f\"  Avg word length: {np.mean([len(t) for t in tokens]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540adad1",
   "metadata": {},
   "source": [
    "## 2. Run Preprocessing on Sample Dataset\n",
    "\n",
    "Run the full preprocessing pipeline on a small subset to verify it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9de59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import preprocess_dataframe\n",
    "\n",
    "data_path = Path(\"../data/IMDB_Dataset.csv\")\n",
    "\n",
    "if data_path.exists():\n",
    "    df_raw = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(df_raw):,} reviews\")\n",
    "    \n",
    "    df_sample = df_raw.head(200).copy()\n",
    "    print(f\"Processing sample of {len(df_sample)} reviews...\")\n",
    "    \n",
    "    df_processed = preprocess_dataframe(\n",
    "        df_sample, \n",
    "        text_col='review', \n",
    "        label_col='sentiment',\n",
    "        keep_punct=True,\n",
    "        pos_sample_limit=200\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nProcessed shape: {df_processed.shape}\")\n",
    "    print(f\"Columns: {list(df_processed.columns)}\")\n",
    "else:\n",
    "    print(f\"ERROR: {data_path} not found. Please download dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457348b5",
   "metadata": {},
   "source": [
    "## 3. Inspect Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6954dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_processed' in dir():\n",
    "    print(\"First 3 rows (excluding tokens/pos_tags):\")\n",
    "    display_cols = ['id', 'text_clean', 'text_length', 'avg_word_len', 'punct_count', 'negation_count', 'ttr', 'label']\n",
    "    print(df_processed[display_cols].head(3))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    print(\"Sample tokens and POS tags for first review:\")\n",
    "    print(f\"Tokens (first 15): {df_processed.iloc[0]['tokens'][:15]}\")\n",
    "    print(f\"POS tags (first 15): {df_processed.iloc[0]['pos_tags'][:15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f45e3",
   "metadata": {},
   "source": [
    "## 4. Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6a1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_processed' in dir():\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    feature_cols = ['text_length', 'avg_word_len', 'punct_count', 'negation_count', 'ttr']\n",
    "    \n",
    "    print(\"Feature Statistics:\")\n",
    "    print(df_processed[feature_cols].describe())\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, col in enumerate(feature_cols):\n",
    "        axes[idx].hist(df_processed[col], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'{col} Distribution', fontsize=11)\n",
    "        axes[idx].set_xlabel(col)\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[5].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380a6fb",
   "metadata": {},
   "source": [
    "## 5. Validation Checks\n",
    "\n",
    "Ensure data integrity after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bac932",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_processed' in dir():\n",
    "    print(\"Validation Checks:\")\n",
    "    print(f\"✓ Row count preserved: {len(df_processed) == len(df_sample)}\")\n",
    "    print(f\"✓ All required columns present: {set(['id', 'text_raw', 'text_clean', 'tokens', 'pos_tags', 'text_length', 'avg_word_len', 'punct_count', 'negation_count', 'ttr', 'label']).issubset(df_processed.columns)}\")\n",
    "    print(f\"✓ No null values in text_clean: {df_processed['text_clean'].notna().all()}\")\n",
    "    print(f\"✓ No null values in tokens: {df_processed['tokens'].notna().all()}\")\n",
    "    print(f\"✓ Label distribution preserved:\")\n",
    "    print(f\"    Original: {df_sample['sentiment'].value_counts().to_dict()}\")\n",
    "    print(f\"    Processed: {df_processed['label'].value_counts().to_dict()}\")\n",
    "    print(f\"✓ Text length matches token count: {(df_processed['text_length'] == df_processed['tokens'].apply(len)).all()}\")\n",
    "    print(f\"✓ TTR in valid range [0,1]: {(df_processed['ttr'] >= 0).all() and (df_processed['ttr'] <= 1).all()}\")\n",
    "    \n",
    "    print(\"\\n✅ All validation checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd620d",
   "metadata": {},
   "source": [
    "## 6. Save Sample Processed Data\n",
    "\n",
    "Save a 200-row sample for the repository (full processed.csv is gitignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_processed' in dir():\n",
    "    df_save = df_processed.copy()\n",
    "    df_save['tokens'] = df_save['tokens'].apply(json.dumps)\n",
    "    df_save['pos_tags'] = df_save['pos_tags'].apply(json.dumps)\n",
    "    \n",
    "    output_path = Path(\"../data/processed_sample.csv\")\n",
    "    df_save.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"✅ Saved {len(df_save)} processed samples to {output_path}\")\n",
    "    print(f\"File size: {output_path.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b50fd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✅ **Phase 2 Preprocessing Complete**\n",
    "\n",
    "**What was done:**\n",
    "- Tested individual preprocessing functions (cleaning, tokenization, POS tagging)\n",
    "- Ran full pipeline on 200-row sample\n",
    "- Validated data integrity (row count, columns, label preservation)\n",
    "- Extracted metadata features: text_length, avg_word_len, punct_count, negation_count, ttr\n",
    "- Saved `processed_sample.csv` for git repository\n",
    "\n",
    "**To process full dataset:**\n",
    "```bash\n",
    "python src/preprocess.py --input data/IMDB_Dataset.csv --output data/processed.csv --pos-limit 5000\n",
    "```\n",
    "\n",
    "Note: Use `--pos-limit` to speed up by only POS-tagging first N rows. Remove for full tagging."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
